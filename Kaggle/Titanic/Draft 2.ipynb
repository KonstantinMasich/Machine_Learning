{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to my OneNote notes.\n",
    "\n",
    "## 0. Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Dataframe operations\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deleting \"PassengerID\", \"Embarked\" and \"Cabin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.drop(['PassengerId', 'Embarked', 'Cabin'], 1, inplace=True)\n",
    "test_df.drop(['PassengerId', 'Embarked', 'Cabin'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding \"Majority\" feature and removing \"Ticket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CREATING NEW FEATURE.\n",
    "\n",
    "# 1. Creating temporary dataframe\n",
    "duplicates = train_df['Ticket'].duplicated(keep=False)\n",
    "duplicates.name = 'Duplicate'\n",
    "temporary_df = pd.concat([train_df['Survived'], train_df['Ticket'], duplicates], axis=1)\n",
    "temporary_df['Majority'] = 0\n",
    "\n",
    "\n",
    "# 2. Constructing Majority feature for TRAIN_DF:\n",
    "majority_list = []\n",
    "for index, row in temporary_df.iterrows():\n",
    "    z = temporary_df.loc[temporary_df['Ticket'] == row['Ticket']]\n",
    "    if row['Duplicate'] == False:\n",
    "        majority_list.append(0)\n",
    "        continue\n",
    "    else:\n",
    "        survived_guys = z['Survived'].sum()\n",
    "        dead_guys = len(z) - survived_guys\n",
    "        if survived_guys >= dead_guys:\n",
    "            majority_list.append(1)\n",
    "        else:\n",
    "            majority_list.append(-1)\n",
    "temporary_df['Majority'] = majority_list\n",
    "\n",
    "# 3. Adding Majority feature to TRAIN_DF:\n",
    "train_df['Majority'] = majority_list\n",
    "\n",
    "# 4. Constructing Majority feature for TEST_DF:\n",
    "majority_list = []\n",
    "for index, row in test_df.iterrows():\n",
    "    # a. Ticket is not encountered in TRAIN dataframe: no info. Set majority = 0.\n",
    "    if row['Ticket'] not in train_df['Ticket'].values:\n",
    "        majority_list.append(0)\n",
    "        continue\n",
    "    # Amounts of occurences of a particular Ticket value in TRAIN and TEST dataframes:\n",
    "    occurences_in_train_df = train_df[train_df['Ticket'] == row['Ticket']].shape[0]\n",
    "    occurences_in_test_df = test_df[test_df['Ticket'] == row['Ticket']].shape[0]\n",
    "    \n",
    "    # b. Ticket is encountered more than once in TRAIN dataframe: set majority accordinlgy. \n",
    "    if occurences_in_train_df > 1:\n",
    "        majority = train_df.loc[train_df['Ticket'] == row['Ticket']]['Majority'].iloc[0]\n",
    "        majority_list.append(majority)\n",
    "    # c. Ticket is encountered once in TRAIN dataframe and more than once in TEST dataframe:\n",
    "    elif occurences_in_train_df == 1 and occurences_in_test_df > 1:\n",
    "        majority_list.append(0)\n",
    "    # d. Ticket is encountered once in TRAIN dataframe and once in TEST dataframe:\n",
    "    elif occurences_in_train_df == 1 and occurences_in_test_df == 1:\n",
    "        survived = train_df.loc[train_df['Ticket'] == row['Ticket']]['Survived'].iloc[0]\n",
    "        if survived:\n",
    "            majority_list.append(1)\n",
    "        else:\n",
    "            majority_list.append(-1)          \n",
    "\n",
    "# 4. Adding Majority feature to TEST_DF:\n",
    "test_df['Majority'] = majority_list\n",
    "\n",
    "# 5. Works is done, removing Ticket feature from both dataframes:\n",
    "train_df.drop('Ticket', 1, inplace=True)\n",
    "test_df.drop('Ticket', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deleting \"Fare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.drop('Fare', 1, inplace=True)\n",
    "test_df.drop('Fare', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. \"Parch\" - ??? {{stored place}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding \"NoSibSp\" and \"SomeSibSp\" features and deleting \"SibSp\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def construct_sib_features(dataframe):\n",
    "#     #dataframe['NoSibSp'] = 0\n",
    "#     #dataframe['SomeSibSp'] = 0\n",
    "#     noSibSp_list = []\n",
    "#     someSibSp_list = []\n",
    "#     for index, row in dataframe.iterrows():\n",
    "#         # SibSp = 0?\n",
    "#         if row['SibSp'] == 0:\n",
    "#             noSibSp_list.append(1)\n",
    "#             someSibSp_list.append(0)\n",
    "#         else:\n",
    "#             noSibSp_list.append(0)\n",
    "#             someSibSp_list.append(row['SibSp'])\n",
    "#     dataframe['NoSibSp'] = noSibSp_list\n",
    "#     dataframe['SomeSibSp'] = someSibSp_list\n",
    "    \n",
    "# construct_sib_features(train_df)\n",
    "# train_df\n",
    "\n",
    "# sns.barplot(x='SomeSibSp', y='Survived', data=train_df)\n",
    "#pd.crosstab([train_df.SibSp],train_df.Survived).style.background_gradient(cmap='summer_r')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Family_Size']=0\n",
    "train_df['Family_Size']=train_df['Parch']+train_df['SibSp']#family size\n",
    "train_df['Alone']=0\n",
    "train_df.loc[train_df.Family_Size==0,'Alone']=1#Alone\n",
    "\n",
    "# f,ax=plt.subplots(1,2,figsize=(18,6))\n",
    "# sns.factorplot('Family_Size','Survived',data=train_df,ax=ax[0])\n",
    "# ax[0].set_title('Family_Size vs Survived')\n",
    "# sns.factorplot('Alone','Survived',data=train_df,ax=ax[1])\n",
    "# ax[1].set_title('Alone vs Survived')\n",
    "# plt.close(2)\n",
    "# plt.close(3)\n",
    "# plt.show()\n",
    "\n",
    "# ---\n",
    "test_df['Family_Size']=0\n",
    "test_df['Family_Size']=test_df['Parch']+test_df['SibSp']#family size\n",
    "test_df['Alone']=0\n",
    "test_df.loc[test_df.Family_Size==0,'Alone']=1#Alone\n",
    "# ---\n",
    "\n",
    "# Dropping SibSp and Parch features\n",
    "train_df.drop(['SibSp', 'Parch'], 1, inplace=True)\n",
    "test_df.drop(['SibSp', 'Parch'], 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Imputing Age\n",
    "Note here that it is better to **use the entire dataset** for mean/median/mode calculation, otherwise we will miss out useful information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "Dr        49.0\n",
       "Master     4.0\n",
       "Miss      22.0\n",
       "Mr        30.0\n",
       "Mrs       36.0\n",
       "Rev       41.5\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEWER\n",
    "\n",
    "# 1. Obtaining the entire data:\n",
    "data = pd.read_csv('train.csv')[['Name', 'Age']].append(pd.read_csv('test.csv')[['Name', 'Age']])\n",
    "data.shape\n",
    "\n",
    "# 2. Constructing Title feature dataframe\n",
    "title_feature = pd.DataFrame()\n",
    "title_feature['Title'] = data['Name']\n",
    "# Cleaning Name feature (deleting quote marks etc., keeping only the letters)\n",
    "for name_string in data['Name']:\n",
    "    title_feature['Title']=data.Name.str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "# Creating relevant mapping:\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
    "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
    "title_feature.replace({'Title': mapping}, inplace=True)\n",
    "title_feature['Age'] = data['Age']\n",
    "title_feature.groupby('Title')['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Median imputation\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Dr'),'Age']     = 49.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Master'),'Age'] = 4.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Miss'),'Age']   = 22.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Mr'),'Age']     = 30.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Mrs'),'Age']    = 36.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Rev'),'Age']    = 41.5\n",
    "\"\"\"\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Dr'),'Age']     = 49.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Master'),'Age'] = 4.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Miss'),'Age']   = 22.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Mr'),'Age']     = 30.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Mrs'),'Age']    = 36.0\n",
    "title_feature.loc[(title_feature['Age'].isnull()) & (title_feature['Title']=='Rev'),'Age']    = 41.5\n",
    "\"\"\"\n",
    "\n",
    "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
    "train_df['Age'] = title_feature['Age'][:891]\n",
    "test_df['Age'] = title_feature['Age'][891:]\n",
    "# print(\"AGE NaN count:\", train_df['Age'].isnull().sum())\n",
    "# print(\"AGE NaN count:\", test_df['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "# =============OLDER VERSION ===============\n",
    "\n",
    "last_names = train_df['Name'].apply(lambda name: name.split(',')[0])\n",
    "import operator\n",
    "\n",
    "last_names = pd.DataFrame(last_names)\n",
    "name_counts = dict(last_names['Name'].value_counts())\n",
    "name_counts = dict(sorted(name_counts.items(), key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "substitution_dict = dict()\n",
    "new_index = 1\n",
    "for k, v in name_counts.items():\n",
    "    if v >= 4:\n",
    "        substitution_dict.update({k: new_index})\n",
    "        new_index += 1\n",
    "    else:\n",
    "        substitution_dict.update({k: 0})\n",
    "\n",
    "family_groups = last_names.replace({'Name': substitution_dict})\n",
    "title_feature = pd.DataFrame()\n",
    "title_feature['Title'] = train_df['Name']\n",
    "for name_string in train_df['Name']:\n",
    "    title_feature['Title']=train_df.Name.str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
    "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss'}\n",
    "title_feature.replace({'Title': mapping}, inplace=True)\n",
    "\n",
    "title_feature['Survived'] = train_df['Survived']\n",
    "#sns.boxplot(x='Title', y='Survived', data=title_feature)\n",
    "title_feature.drop('Survived', axis=1, inplace=True)\n",
    "title_feature['Age'] = train_df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE NaN count: 0\n",
      "AGE NaN count: 0\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['Title'] = title_feature['Title']\n",
    "temp['Age'] = train_df['Age'] \n",
    "\n",
    "# Forming titles DFs\n",
    "titles = list(set(temp['Title'].tolist())) # List of all our chosen titles\n",
    "l = []\n",
    "for t in titles:\n",
    "    l.append(temp.loc[(temp.Age.notnull()) & (temp['Title']==t)])\n",
    "\n",
    "# Note that we perform imputation on TRAIN_DF now!\n",
    "train_df['Title'] = title_feature['Title']\n",
    "train_df.loc[(train_df['Age'].isnull()) & (train_df['Title']=='Dr'),'Age']     = 46.5\n",
    "train_df.loc[(train_df['Age'].isnull()) & (train_df['Title']=='Master'),'Age'] = 3.5\n",
    "train_df.loc[(train_df['Age'].isnull()) & (train_df['Title']=='Miss'),'Age']   = 21.5\n",
    "train_df.loc[(train_df['Age'].isnull()) & (train_df['Title']=='Mr'),'Age']     = 30.0\n",
    "train_df.loc[(train_df['Age'].isnull()) & (train_df['Title']=='Mrs'),'Age']    = 35.9\n",
    "train_df.loc[(train_df['Age'].isnull()) & (train_df['Title']=='Rev'),'Age']    = 46.5\n",
    "\n",
    "# Dropping Title feature for now - because maybe it will not be needed at all, who knows.\n",
    "train_df.drop('Title', axis=1, inplace=True)\n",
    "\n",
    "print(\"AGE NaN count:\", train_df['Age'].isnull().sum())\n",
    "\n",
    "# Note that we perform imputation on TRAIN_DF now!\n",
    "test_df['Title'] = title_feature['Title']\n",
    "test_df.loc[(test_df['Age'].isnull()) & (test_df['Title']=='Dr'),'Age']     = 46.5\n",
    "test_df.loc[(test_df['Age'].isnull()) & (test_df['Title']=='Master'),'Age'] = 3.5\n",
    "test_df.loc[(test_df['Age'].isnull()) & (test_df['Title']=='Miss'),'Age']   = 21.5\n",
    "test_df.loc[(test_df['Age'].isnull()) & (test_df['Title']=='Mr'),'Age']     = 30.0\n",
    "test_df.loc[(test_df['Age'].isnull()) & (test_df['Title']=='Mrs'),'Age']    = 35.9\n",
    "test_df.loc[(test_df['Age'].isnull()) & (test_df['Title']=='Rev'),'Age']    = 46.5\n",
    "\n",
    "# Dropping Title feature for now - because maybe it will not be needed at all, who knows.\n",
    "test_df.drop('Title', axis=1, inplace=True)\n",
    "print(\"AGE NaN count:\", test_df['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modifying Sex - replacing male/female with 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "test_df['Sex'].replace(['male','female'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deleting \"Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.drop('Name', 1, inplace=True)\n",
    "test_df.drop('Name', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "# ======================== TRAINING! ========================\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ? Preprocessing? Scaling? ##\n",
    "\n",
    "### 0. Загрузка данных из датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_df.drop('Survived', 1)\n",
    "y = train_df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Шкалирование в [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = mm_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# std_scaler = StandardScaler().fit(X)\n",
    "# X = std_scaler.transform(X)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X = std_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing all the required ML packages\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_df.drop('Survived', 1)\n",
    "y = train_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validated score is: 0.841769379185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "knn = KNeighborsClassifier(n_neighbors=8, weights='distance')\n",
    "scores = cross_val_score(knn, X, y, scoring='accuracy', cv=10)\n",
    "print(\"Mean cross-validated score is:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is: 0.838436045852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Logistic regression\n",
    "log_regression = LogisticRegression()\n",
    "scores = cross_val_score(log_regression, X, y, scoring='accuracy', cv=10)\n",
    "print(\"Mean is:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,test=train_test_split(train_df,test_size=0.3,random_state=0,stratify=train_df['Survived'])\n",
    "train_X=train[train.columns[1:]]\n",
    "train_Y=train[train.columns[:1]]\n",
    "test_X=test[test.columns[1:]]\n",
    "test_Y=test[test.columns[:1]]\n",
    "X=train_df[train_df.columns[1:]]\n",
    "Y=train_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SCALING AGAIN - DELETE ME IF NECESSARY\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = StandardScaler()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "X = rescaledX\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "test_Y = scaler.fit_transform(test_Y)\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "train_Y = scaler.fit_transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-78ba8bb61482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprediction1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy for rbf SVM is '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction1=model.predict(test_X)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-ca253ac3f52c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprediction2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy for linear SVM is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction2=model.predict(test_X)\n",
    "print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Svm</th>\n",
       "      <td>0.832747</td>\n",
       "      <td>0.028307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radial Svm</th>\n",
       "      <td>0.851835</td>\n",
       "      <td>0.033319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.840624</td>\n",
       "      <td>0.025045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.846255</td>\n",
       "      <td>0.033273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.829388</td>\n",
       "      <td>0.023026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.824931</td>\n",
       "      <td>0.028845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.835044</td>\n",
       "      <td>0.018552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CV Mean       Std\n",
       "Linear Svm           0.832747  0.028307\n",
       "Radial Svm           0.851835  0.033319\n",
       "Logistic Regression  0.840624  0.025045\n",
       "KNN                  0.846255  0.033273\n",
       "Decision Tree        0.829388  0.023026\n",
       "Naive Bayes          0.824931  0.028845\n",
       "Random Forest        0.835044  0.018552"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "xyz=[]\n",
    "accuracy=[]\n",
    "std=[]\n",
    "classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\n",
    "models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result=cv_result\n",
    "    xyz.append(cv_result.mean())\n",
    "    std.append(cv_result.std())\n",
    "    accuracy.append(cv_result)\n",
    "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \n",
    "new_models_dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/konstantin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-6c8dbd3951c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                              ], \n\u001b[1;32m     10\u001b[0m                        voting='soft').fit(train_X,train_Y)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The accuracy for ensembled model is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mensemble_lin_rbf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_lin_rbf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The cross validated score is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n",
    "                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n",
    "                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n",
    "                                              ('LR',LogisticRegression(C=0.05)),\n",
    "                                              ('DT',DecisionTreeClassifier(random_state=0)),\n",
    "                                              ('NB',GaussianNB()),\n",
    "                                              ('svm',svm.SVC(kernel='linear',probability=True))\n",
    "                                             ], \n",
    "                       voting='soft').fit(train_X,train_Y)\n",
    "print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\n",
    "cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\n",
    "print('The cross validated score is',cross.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validated score for AdaBoost is: 0.844066791511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\n",
    "result=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoost is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849607182941\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.05, n_estimators=200, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_estimators=list(range(100,1100,100))\n",
    "learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\n",
    "gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validated score for AdaBoost is: 0.847387356713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.05, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_df.drop('Survived', 1)\n",
    "y = train_df['Survived']\n",
    "\n",
    "model = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
    "          learning_rate=0.05, n_estimators=200, random_state=None)\n",
    "result=cross_val_score(model, X, Y, cv=10, scoring='accuracy')\n",
    "print('The cross validated score for AdaBoost is:',result.mean())\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = test_df\n",
    "x_hat = model.predict(X)\n",
    "\n",
    "temp = pd.read_csv('test.csv')[['PassengerId', 'Sex']]\n",
    "temp['Survived'] = x_hat\n",
    "temp.drop('Sex', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.to_csv(\"z.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br><br>\n",
    "\n",
    "# АППЕНДИКС\n",
    "Записать текущие **Pandas-датафреймы TRAIN_DF и TEST_DF** в файлы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df.csv\", index=False)\n",
    "test_df.to_csv(\"test_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
